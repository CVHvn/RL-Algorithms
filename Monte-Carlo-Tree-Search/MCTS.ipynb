{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a358b1fe",
   "metadata": {},
   "source": [
    "# Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "387b6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ddf298",
   "metadata": {},
   "source": [
    "# Config\n",
    "Just use a global variable for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ecdde4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"CartPole-v1\"\n",
    "TOTAL_EPISODE = 10\n",
    "GAMMA = 0.99\n",
    "NUM_ACTION = 2\n",
    "REUSE_TREE = False #reuse or build new tree. After first MCTS (for s0), we have tree for s1, ... We can reuse this tree or build new tree for s1, s2, ...\n",
    "#You need set TIMEOUT or SEARCH_STEP to None and another to integer\n",
    "TIMEOUT = None #each MCTS step will run until timeout (set TIMEOUT to None if you don't want limit by time)\n",
    "SEARCH_STEP = 20 #each MCTS step will run SEARCH_STEP (set SEARCH_STEP to None if you want to run with time limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d530f8de",
   "metadata": {},
   "source": [
    "# MCTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d28eaf",
   "metadata": {},
   "source": [
    "## Node\n",
    "The Node includes two main functions: init and get_score:\n",
    "- init:\n",
    "    - N: number of visits, initialized to 0\n",
    "    - V: sum of returns, initialized to 0\n",
    "    - children: None initially; children nodes will be added in the explore function\n",
    "    - parent: the parent node, or None if this is the root\n",
    "    - lead_action: the action that led to this node, or None if root\n",
    "    - num_action: number of possible actions\n",
    "    - env: a copy of the environment from the parent node’s env (or copy the main env if root)\n",
    "    - If lead_action exists: perform this action to get reward and done status\n",
    "    - Otherwise, set reward = 0 and done = False\n",
    "    - available_actions: the set of legal actions (equal to num_action), from which actions will be removed when children nodes are added\n",
    "- get_score: \n",
    "    - Calculate the score of the node using the formula: \n",
    "        - If the node is root, score = 0\n",
    "        - Otherwise: $score = \\frac{V}{N} + c \\sqrt\\frac{2\\ln(N_p)}{N}$\n",
    "    - Where $c = \\frac{1}{\\sqrt 2}$\n",
    "    - $N_p$ is the visit count of the parent node if the node has a parent, or the node’s own visit count if it is the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0770f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, num_action, base_env, parent = None, lead_action = None, state = None):\n",
    "        self.N = 0\n",
    "        self.V = 0.\n",
    "\n",
    "        self.children = None\n",
    "        self.parent = parent\n",
    "        self.lead_action = lead_action\n",
    "        self.num_action = num_action\n",
    "\n",
    "        self.env = copy.deepcopy(base_env)\n",
    "        if lead_action is not None:\n",
    "            self.state, self.reward, terminated, truncated, info = self.env.step(lead_action)\n",
    "            self.done = terminated or truncated\n",
    "        else:\n",
    "            self.done = False\n",
    "            self.reward = 0.\n",
    "            self.state = state\n",
    "        self.available_actions = set(list(range(num_action))) if not self.done else set()\n",
    "\n",
    "    def get_score(self):\n",
    "        if self.N == 0:\n",
    "            return 1e9\n",
    "\n",
    "        top_node = self\n",
    "        if top_node.parent is not None:\n",
    "            top_node = top_node.parent\n",
    "\n",
    "        c = 1. / np.sqrt(2)\n",
    "        V = self.V / self.N\n",
    "        return V + c * np.sqrt(2 * np.log(top_node.N) / self.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca1cd1",
   "metadata": {},
   "source": [
    "## MCTS functions\n",
    "\n",
    "**simulate**: Perform a random play (rollout) for one episode and return the total reward of that episode.\n",
    "- Copy the environment state from the given node before simulation.\n",
    "\n",
    "**explore**:\n",
    "- Select a leaf node starting from the root by repeating:\n",
    "    - Use the score function to select the best child node.\n",
    "    - Move to that child node.\n",
    "    - Stop when reaching a terminal node or a node that is not fully expanded.\n",
    "- If the current node is not fully expanded:\n",
    "    - Randomly select an action from the available actions.\n",
    "    - Create a child node for this action.\n",
    "    - Move to this child node.\n",
    "- Perform simulation from this node to get a return value.\n",
    "- Backpropagate the return value up the tree.\n",
    "\n",
    "**mcts**: \n",
    "- Perform multiple explore steps.\n",
    "- Choose the best action as the child node with the highest visit count (N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75b3ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(node):\n",
    "    env = copy.deepcopy(node.env)\n",
    "    done = node.done\n",
    "    G = 0.\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        action = np.random.choice(NUM_ACTION)\n",
    "        _, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        rewards.append(reward)\n",
    "    for reward in reversed(rewards):\n",
    "        G = G * GAMMA + reward\n",
    "    env.close()\n",
    "    return G\n",
    "\n",
    "def explore(explore_node):\n",
    "    current_node = explore_node\n",
    "    while current_node.children and not current_node.done:\n",
    "        children = current_node.children\n",
    "        if len(children) < NUM_ACTION:\n",
    "            break\n",
    "        actions_score = [child.get_score() for child in children]\n",
    "        max_score = max(actions_score)\n",
    "        best_children = [child for child, score in zip(children, actions_score) if score == max_score]\n",
    "        idx = np.random.choice(len(best_children))\n",
    "        current_node = best_children[idx]\n",
    "    available_actions = current_node.available_actions\n",
    "    if len(available_actions) > 0:\n",
    "        action = np.random.choice(list(available_actions))\n",
    "        current_node.available_actions.remove(action)\n",
    "        new_node = Node(NUM_ACTION, current_node.env, current_node, action, None)\n",
    "        if current_node.children is not None:\n",
    "            current_node.children.append(new_node)\n",
    "        else:\n",
    "            current_node.children = [new_node]\n",
    "        current_node = new_node\n",
    "    G = simulate(current_node)\n",
    "    while current_node:\n",
    "        current_node.N += 1\n",
    "        current_node.V += G\n",
    "        G = GAMMA * G + current_node.reward\n",
    "        current_node = current_node.parent\n",
    "\n",
    "def check_mcts(timeout, start_time, search_step, current_step):\n",
    "    if timeout is None:\n",
    "        return current_step < search_step\n",
    "    return datetime.now() - start_time < timedelta(seconds=timeout)\n",
    "\n",
    "def mcts(root_state, root_env, timeout = 1, search_step = None, tree=None):\n",
    "    root_node = tree if (tree and REUSE_TREE) else Node(NUM_ACTION, root_env, None, None)\n",
    "    start_time = datetime.now()\n",
    "    step = 0\n",
    "    while check_mcts(timeout, start_time, search_step, step):\n",
    "        explore(root_node)\n",
    "        step += 1\n",
    "    children = root_node.children\n",
    "    Ns= [child.N for child in children]\n",
    "    max_N = max(Ns)\n",
    "    best_children = [child for child, action, N in zip(children, [child.lead_action for child in children], Ns) if N == max_N]\n",
    "    best_actions = [action for child, action, N in zip(children, [child.lead_action for child in children], Ns) if N == max_N]\n",
    "    best_child = best_children[0]\n",
    "    best_child.parent = None\n",
    "    best_child.lead_action = None\n",
    "    best_child.reward = 0.\n",
    "    best_child.done = False\n",
    "    return best_actions[0], best_child"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97153b",
   "metadata": {},
   "source": [
    "# play and test episodes\n",
    "\n",
    "Play TOTAL_EPISODE episodes and print the results. For each episode:\n",
    "- Reset the environment.\n",
    "- Repeat until the episode ends:\n",
    "    - Use the mcts function to find the best action.\n",
    "    - Perform this action in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcf0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "episode_runtimes = []\n",
    "\n",
    "for episode in range(TOTAL_EPISODE):\n",
    "    start_time = datetime.now()\n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_step = 0\n",
    "    next_tree = None\n",
    "\n",
    "    while not done:\n",
    "        action, next_tree = mcts(state, env, TIMEOUT, SEARCH_STEP, next_tree)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_step += 1\n",
    "        if episode_step % 100 == 0:\n",
    "            print(episode_step, episode_reward)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_steps.append(episode_step)\n",
    "    episode_runtimes.append(datetime.now() - start_time)\n",
    "    print(episode_reward, episode_runtimes[-1], \"\\n\")\n",
    "\n",
    "episode_rewards = np.array(episode_rewards)\n",
    "print(episode_rewards)\n",
    "print(episode_rewards.max(), episode_rewards.min(), episode_rewards.mean(), episode_rewards.std())\n",
    "print(max(episode_runtimes), min(episode_runtimes), np.mean(episode_runtimes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
