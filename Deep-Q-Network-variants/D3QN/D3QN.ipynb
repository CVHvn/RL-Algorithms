{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf627f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        # env\n",
    "        self.env_name = \"CartPole-v1\"\n",
    "        self.gamma = 0.99\n",
    "        self.num_action = 2\n",
    "        self.state_dim = 4\n",
    "\n",
    "        # replay\n",
    "        self.buffer_size = 100000\n",
    "        self.batch_size = 64\n",
    "\n",
    "        # training\n",
    "        self.total_episodes = 2000\n",
    "        self.learning_rate = 2.3e-3\n",
    "        self.weight_decay = 1e-4\n",
    "        self.start_training_step = 1000\n",
    "        self.train_frequency = 4\n",
    "        self.epochs = 1\n",
    "        self.test_frequency = 10\n",
    "        self.num_test_episodes = 10\n",
    "        self.save_frequency = 1000\n",
    "        self.save_path = 'best_model.pth'\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        ## target network\n",
    "        self.use_soft_update = True\n",
    "        self.update_frequency = 100\n",
    "        self.tau = 0.005\n",
    "\n",
    "        ## episode\n",
    "        self.init_epsilon = 1.\n",
    "        self.end_epsilon = 0.04\n",
    "        self.exploration_fraction = 0.16\n",
    "        self.decay_step = self.total_episodes * self.exploration_fraction\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bcfe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_Buffer():\n",
    "    def __init__(self, buffer_size, state_dim):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.real_size = 0\n",
    "        self.index = 0\n",
    "\n",
    "        self.states = np.zeros((buffer_size, state_dim))\n",
    "        self.actions = np.zeros((buffer_size,))\n",
    "        self.rewards = np.zeros((buffer_size,))\n",
    "        self.dones = np.zeros((buffer_size,), dtype = bool)\n",
    "        self.next_states = np.zeros((buffer_size, state_dim))\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states[self.index] = state\n",
    "        self.actions[self.index] = action\n",
    "        self.rewards[self.index] = reward\n",
    "        self.next_states[self.index] = next_state\n",
    "        self.dones[self.index] = done\n",
    "\n",
    "        self.real_size = min(self.real_size+1, self.buffer_size)\n",
    "        self.index = (self.index+1) % self.buffer_size\n",
    "\n",
    "    def sample(self, batchsize):\n",
    "        idxs = np.random.choice(self.real_size, batchsize, replace=False).astype(np.int64)\n",
    "        return torch.tensor(self.states[idxs]).float(), \\\n",
    "                torch.tensor(self.actions[idxs]).long().reshape(-1), \\\n",
    "                    torch.tensor(self.rewards[idxs]).reshape(-1), \\\n",
    "                        torch.tensor(self.next_states[idxs]).float(), torch.tensor(self.dones[idxs].astype(np.float32)).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, state_dim=4, num_action=2):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.V = nn.Linear(64, 1)\n",
    "        self.A = nn.Linear(64, num_action)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        A = self.A(x)\n",
    "        V = self.V(x)\n",
    "        Q = V + A - A.mean(-1, keepdims=True)\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f25a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, model, target_model, env, env_name, replay_buffer, num_action, batch_size, learning_rate, weight_decay, gamma, total_episodes,\n",
    "                 train_frequency, epochs, test_frequency, save_frequency, num_test_episodes, start_training_step, save_path, use_soft_update, \n",
    "                 tau, init_epsilon, decay_step, end_epsilon, update_frequency, device):\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.target_model = target_model\n",
    "        self.use_soft_update = use_soft_update\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.update_frequency = update_frequency\n",
    "\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.gamma = gamma\n",
    "        self.num_action = num_action\n",
    "\n",
    "        self.total_episodes = total_episodes\n",
    "        self.train_frequency = train_frequency\n",
    "        self.epochs = epochs\n",
    "        self.test_frequency = test_frequency\n",
    "        self.save_frequency = save_frequency\n",
    "        self.num_test_episodes = num_test_episodes\n",
    "        self.save_path = save_path\n",
    "        self.start_training_step = start_training_step\n",
    "\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.decay_step = decay_step\n",
    "        self.end_epsilon = end_epsilon\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def update_weights(self):\n",
    "        if self.use_soft_update:\n",
    "            with torch.no_grad():\n",
    "                for target_param, online_param in zip(self.target_model.parameters(), self.model.parameters()):\n",
    "                    target_param.data.copy_(self.tau * online_param.data + (1.0 - self.tau) * target_param.data)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                for target_param, online_param in zip(self.target_model.parameters(), self.model.parameters()):\n",
    "                    target_param.data.copy_(online_param.data)\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        state = torch.tensor(np.array(state)).unsqueeze(0).to(self.device)\n",
    "        if np.random.rand() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                Qs = self.model(state)[0]\n",
    "                action = Qs.argmax().item()\n",
    "        else:\n",
    "            action = np.random.choice(self.num_action, 1)[0]\n",
    "        return action\n",
    "    \n",
    "    def decay_epsilon(self, step):\n",
    "        if step < self.decay_step:\n",
    "            epsilon = self.init_epsilon - (step / self.decay_step) * (self.init_epsilon - self.end_epsilon)\n",
    "        else:\n",
    "            epsilon = self.end_epsilon\n",
    "        return epsilon\n",
    "    \n",
    "    def train(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        idxs = torch.arange(0, self.batch_size)\n",
    "        with torch.no_grad():\n",
    "            next_Qs_online = self.model(next_states.to(self.device))\n",
    "            next_actions = next_Qs_online.argmax(-1)\n",
    "            \n",
    "            next_Qs = self.target_model(next_states.to(self.device))\n",
    "            next_Qs = next_Qs[idxs, next_actions].reshape(-1)\n",
    "        targets = rewards.to(self.device) + self.gamma * (1 - dones.to(self.device)) * next_Qs.to(self.device)\n",
    "        Qs = self.model(states.to(self.device))[idxs, actions].reshape(-1)\n",
    "        loss = ((targets - Qs)**2).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def test(self):\n",
    "        env = gym.make(self.env_name, render_mode=\"rgb_array\")\n",
    "        state, info = env.reset()\n",
    "        total_rewards = 0.\n",
    "        step = 0.\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.select_action(state, -1)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_rewards += reward\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        if total_rewards > self.max_test_rewards:\n",
    "            self.max_test_rewards = total_rewards\n",
    "            torch.save(self.model.state_dict(), self.save_path)\n",
    "        \n",
    "        env.close()\n",
    "        return total_rewards, step\n",
    "    \n",
    "    def learn(self):\n",
    "        self.update_weights()\n",
    "        self.current_step = 0\n",
    "        epsilon = self.init_epsilon\n",
    "\n",
    "        self.max_test_rewards = 0.\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "        self.max_episode_rewards = 0.\n",
    "        self.test_episode_rewards = []\n",
    "        self.test_episode_steps = []\n",
    "        self.losses = []\n",
    "\n",
    "        for episode in range(1, self.total_episodes+1):\n",
    "            start_time = datetime.now()\n",
    "            state, info = self.env.reset()\n",
    "            done = False\n",
    "            total_rewards = 0.\n",
    "            episode_step = 0.\n",
    "            while not done:\n",
    "                self.current_step += 1\n",
    "                episode_step += 1\n",
    "                action = self.select_action(state, epsilon)\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                total_rewards += reward\n",
    "                done = terminated or truncated\n",
    "                self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                if self.current_step % self.train_frequency == 0 and self.current_step >= self.start_training_step:\n",
    "                    for _ in range(self.epochs):\n",
    "                        loss = self.train()\n",
    "                        self.losses.append(loss)\n",
    "                if self.current_step % self.update_frequency == 0 or self.use_soft_update:\n",
    "                    self.update_weights()\n",
    "\n",
    "            self.episode_rewards.append(total_rewards)\n",
    "            self.episode_steps.append(episode_step)\n",
    "            self.max_episode_rewards = max(self.max_episode_rewards, total_rewards)\n",
    "            if episode % 100 == 0:\n",
    "                print(f\"Step {self.current_step}, Episode {episode}: Steps: {episode_step}, Rewards: {total_rewards}, \"\n",
    "                      f\"Mean_Rewards: {np.array(self.episode_rewards[-min(100, len(self.episode_rewards)):]).mean():.4f}, \"\n",
    "                      f\"Max_Rewards: {self.max_episode_rewards}, Loss: {np.array(self.losses[-min(1000, len(self.losses)):]).mean():.4f}, \"\n",
    "                      f\"Duration: {datetime.now() - start_time}, epsilon: {epsilon:.6f}\")        \n",
    "\n",
    "            if episode % self.test_frequency == 0:\n",
    "                start_time = datetime.now()\n",
    "                for _ in range(self.num_test_episodes):\n",
    "                    test_rewards, test_steps = self.test()\n",
    "                    self.test_episode_rewards.append(test_rewards)\n",
    "                    self.test_episode_steps.append(test_steps)\n",
    "                if episode % 100 == 0:\n",
    "                    print(f\"Test Episode: {len(self.test_episode_rewards)} Mean Rewards: {np.array(self.test_episode_rewards[-self.num_test_episodes:]).mean():.4f}, \"\n",
    "                          f\"Max_Test_Rewards: {self.max_test_rewards}, Duration: {datetime.now() - start_time}\")\n",
    "            if episode % self.save_frequency == 0:\n",
    "                torch.save(self.model.state_dict(), f\"{episode}.pth\")\n",
    "            epsilon = self.decay_epsilon(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a6406",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = Replay_Buffer(config.buffer_size, config.state_dim)\n",
    "model = Model(config.state_dim).to(config.device)\n",
    "target_model = Model(config.state_dim).to(config.device)\n",
    "env = gym.make(config.env_name, render_mode=\"rgb_array\")\n",
    "\n",
    "agent = Agent(model, target_model, env, config.env_name, replay_buffer, config.num_action, config.batch_size, config.learning_rate, config.weight_decay, \n",
    "              config.gamma, config.total_episodes, config.train_frequency, config.epochs, config.test_frequency, config.save_frequency, \n",
    "              config.num_test_episodes, config.start_training_step, config.save_path, config.use_soft_update, config.tau, \n",
    "              config.init_epsilon, config.decay_step, config.end_epsilon, config.update_frequency, config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d930d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episode_rewards = agent.episode_rewards\n",
    "test_episode_rewards = agent.test_episode_rewards\n",
    "\n",
    "train_episodes = list(range(1, len(agent.episode_rewards)+1))\n",
    "test_episodes = list(range(1, len(agent.test_episode_rewards)+1, 1))\n",
    "test_episodes_per_test = list(range(agent.test_frequency, len(agent.test_episode_rewards)+1, agent.test_frequency))\n",
    "\n",
    "episode_rewards = np.array(episode_rewards)\n",
    "test_episode_rewards = np.array(test_episode_rewards)\n",
    "\n",
    "mean_rewards = []\n",
    "mean_test_rewards_per_test = []\n",
    "mean_test_rewards_l100 = []\n",
    "for i in range(len(episode_rewards)):\n",
    "    mean_rewards.append(episode_rewards[max(0, i-100):i+1].mean())\n",
    "for i in range(len(test_episode_rewards)):\n",
    "    if (i+1) % config.num_test_episodes == 0:\n",
    "        mean_test_rewards_per_test.append(test_episode_rewards[max(0, i-config.num_test_episodes):i+1].mean())\n",
    "    mean_test_rewards_l100.append(test_episode_rewards[max(0, i-100):i+1].mean())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "idx_full_rewards = np.array(test_episode_rewards) == 500\n",
    "idx_full_mean_rewards = np.array(mean_test_rewards_per_test) == 500\n",
    "\n",
    "plt.plot(train_episodes, mean_rewards, label='Train Rewards [Last 100]')\n",
    "plt.plot(test_episodes, mean_test_rewards_l100, label='Test Rewards [Last 100]')\n",
    "plt.scatter(np.array(test_episodes)[idx_full_rewards], \n",
    "            np.array(test_episode_rewards)[idx_full_rewards], label='Test Rewards = 500', alpha=0.6, s=5, color = 'green')\n",
    "plt.scatter(np.array(test_episodes_per_test)[idx_full_mean_rewards], \n",
    "            np.array(mean_test_rewards_per_test)[idx_full_mean_rewards], label='Test Rewards [Last 10] = 500', alpha=0.6, s=50, color = 'red')\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"D3QN\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
