{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58b9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf627f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        # env\n",
    "        self.env_name = \"CartPole-v1\"\n",
    "        self.gamma = 0.99\n",
    "        self.num_action = 2\n",
    "        self.state_dim = 4\n",
    "\n",
    "        # replay\n",
    "        self.buffer_size = 100000\n",
    "        self.batchsize = 64\n",
    "\n",
    "        # training\n",
    "        self.total_episodes = 500000\n",
    "        self.total_steps = 500000\n",
    "        self.learning_rate = 2.3e-3\n",
    "        self.weight_decay = 1e-4\n",
    "        self.start_training_step = 1000\n",
    "        self.train_frequency = 256\n",
    "        self.epochs = 128\n",
    "        self.test_frequency = 10000\n",
    "        self.save_frequency = 50000\n",
    "        self.save_path = 'best_model.pth'\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        ## target network\n",
    "        self.use_soft_update = True\n",
    "        self.update_frequency = 10\n",
    "        self.tau = 0.005\n",
    "\n",
    "        ## episode\n",
    "        self.init_epsilon = 1.\n",
    "        self.end_epsilon = 0.04\n",
    "        self.exploration_fraction = 0.16\n",
    "        self.decay_step = self.total_episodes * self.exploration_fraction\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61bcfe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_Buffer():\n",
    "    def __init__(self, buffer_size, state_dim):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.real_size = 0\n",
    "        self.index = 0\n",
    "\n",
    "        self.states = np.zeros((buffer_size, state_dim))\n",
    "        self.actions = np.zeros((buffer_size,))\n",
    "        self.rewards = np.zeros((buffer_size,))\n",
    "        self.dones = np.zeros((buffer_size,), dtype = bool)\n",
    "        self.next_states = np.zeros((buffer_size, state_dim))\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states[self.index] = state\n",
    "        self.actions[self.index] = action\n",
    "        self.rewards[self.index] = reward\n",
    "        self.next_states[self.index] = next_state\n",
    "        self.dones[self.index] = done\n",
    "\n",
    "        self.real_size = min(self.real_size+1, self.buffer_size)\n",
    "        self.index = (self.index+1) % self.buffer_size\n",
    "\n",
    "    def sample(self, batchsize):\n",
    "        idxs = np.random.choice(self.real_size, batchsize, replace=False).astype(np.int64)\n",
    "        return torch.tensor(self.states[idxs]).float(), \\\n",
    "                torch.tensor(self.actions[idxs]).long().reshape(-1), \\\n",
    "                    torch.tensor(self.rewards[idxs]).reshape(-1), \\\n",
    "                        torch.tensor(self.next_states[idxs]).float(), torch.tensor(self.dones[idxs].astype(np.float32)).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, state_dim=4, num_action=2):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.V = nn.Linear(64, 1)\n",
    "        self.A = nn.Linear(64, num_action)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        A = self.A(x)\n",
    "        V = self.V(x)\n",
    "        Q = V + A - A.mean(-1, keepdims=True)\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e59339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(model, target_model, use_soft_update, tau):\n",
    "    if use_soft_update:\n",
    "        with torch.no_grad():\n",
    "            for target_param, online_param in zip(target_model.parameters(), model.parameters()):\n",
    "                target_param.data.copy_(tau * online_param.data + (1.0 - tau) * target_param.data)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            for target_param, online_param in zip(target_model.parameters(), model.parameters()):\n",
    "                target_param.data.copy_(online_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54caa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, model, epsilon, num_action, config):\n",
    "    state = torch.tensor(np.array(state)).unsqueeze(0).to(config.device)\n",
    "    if np.random.rand() > epsilon:\n",
    "        with torch.no_grad():\n",
    "            Qs = model(state)[0]\n",
    "            action = Qs.argmax().item()\n",
    "    else:\n",
    "        action = np.random.choice(num_action, 1)[0]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0cf6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_epsilon(step, config):\n",
    "    if step < config.decay_step:\n",
    "        epsilon = config.init_epsilon - (step / config.decay_step) * (config.init_epsilon - config.end_epsilon)\n",
    "    else:\n",
    "        epsilon = config.end_epsilon\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab8828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, target_model, replay_buffer, batchsize, gamma, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batchsize)\n",
    "    idxs = torch.arange(0, batchsize)\n",
    "    with torch.no_grad():\n",
    "        next_Qs = target_model(next_states.to(config.device))\n",
    "        next_actions = next_Qs.argmax(-1)\n",
    "        next_Qs = next_Qs[idxs, next_actions].reshape(-1)\n",
    "    targets = rewards.to(config.device) + gamma * (1 - dones.to(config.device)) * next_Qs.to(config.device)\n",
    "    Qs = model(states.to(config.device))[idxs, actions].reshape(-1)\n",
    "    loss = ((targets - Qs)**2).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "    \n",
    "def test(config, model, max_test_rewards):\n",
    "    env = gym.make(config.env_name, render_mode=\"rgb_array\")\n",
    "    state, info = env.reset()\n",
    "    total_rewards = 0.\n",
    "    step = 0.\n",
    "    start_time = datetime.now()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = select_action(state, model, -1, config.num_action, config)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_rewards += reward\n",
    "        state = next_state\n",
    "        step += 1\n",
    "    if total_rewards > max_test_rewards:\n",
    "        max_test_rewards = total_rewards\n",
    "        torch.save(model.state_dict(), config.save_path)\n",
    "    print(f\"Test Episode: Steps: {step} Rewards: {total_rewards}, Max_Test_Rewards: {max_test_rewards}, Duration: {datetime.now() - start_time}\")\n",
    "    return max_test_rewards, total_rewards, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d930d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = Replay_Buffer(config.buffer_size, config.state_dim)\n",
    "model = Model(config.state_dim).to(config.device)\n",
    "target_model = Model(config.state_dim).to(config.device)\n",
    "update_weights(model, target_model, config.use_soft_update, config.tau)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "step = 0\n",
    "env = gym.make(config.env_name, render_mode=\"rgb_array\")\n",
    "epsilon = config.init_epsilon\n",
    "max_test_rewards = 0.\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "max_episode_rewards = 0.\n",
    "test_episode_rewards = []\n",
    "test_episode_steps = []\n",
    "losses = []\n",
    "\n",
    "for episode in range(config.total_episodes):\n",
    "    if config.total_steps is not None and step > config.total_steps:\n",
    "        break\n",
    "    start_time = datetime.now()\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    total_rewards = 0.\n",
    "    episode_step = 0.\n",
    "    while not done:\n",
    "        step += 1\n",
    "        episode_step += 1\n",
    "        action = select_action(state, model, epsilon, config.num_action, config)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_rewards += reward\n",
    "        done = terminated or truncated\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if step % config.train_frequency == 0 and step >= config.start_training_step:\n",
    "            for _ in range(config.epochs):\n",
    "                loss = train(model, target_model, replay_buffer, config.batchsize, config.gamma, optimizer)\n",
    "                losses.append(loss)\n",
    "        if step % config.test_frequency == 0:\n",
    "            max_test_rewards, test_rewards, test_steps = test(config, model, max_test_rewards)\n",
    "            test_episode_rewards.append(test_rewards)\n",
    "            test_episode_steps.append(test_steps)\n",
    "        if step % config.save_frequency == 0:\n",
    "            torch.save(model.state_dict(), f\"{step}.pth\")\n",
    "        epsilon = decay_epsilon(step, config)\n",
    "\n",
    "        if step % config.update_frequency or config.use_soft_update:\n",
    "            update_weights(model, target_model, config.use_soft_update, config.tau)\n",
    "    episode_rewards.append(total_rewards)\n",
    "    episode_steps.append(episode_step)\n",
    "    max_episode_rewards = max(max_episode_rewards, total_rewards)\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Step {step}, Episode {episode}: Steps: {episode_step}, Rewards: {total_rewards}, Mean_Rewards: {np.array(episode_rewards[-min(100, len(episode_rewards)):]).mean():.4f}, Max_Rewards: {max_episode_rewards}, Loss: {np.array(losses[-min(1000, len(losses)):]).mean()}, Duration: {datetime.now() - start_time}, epsilon: {epsilon:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f94aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open(\"log.pickle\", \"wb\") as f:\n",
    "    pickle.dump({\"rewards\": np.array(episode_rewards),\n",
    "                 \"steps\": np.array(episode_steps),\n",
    "                 \"test rewards\": np.array(test_episode_rewards),\n",
    "                 \"test steps\": np.array(test_episode_steps),\n",
    "                 \"losses\": np.array(losses)}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe2fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_steps = np.arange(config.test_frequency, config.total_steps+1, config.test_frequency)\n",
    "train_step = 0\n",
    "train_steps = []\n",
    "for steps in episode_steps:\n",
    "    train_step += steps\n",
    "    train_steps.append(train_step)\n",
    "print(len(test_steps), len(test_episode_rewards))\n",
    "\n",
    "mean_rewards = []\n",
    "mean_rewards_last10 = []\n",
    "mean_test_rewards = []\n",
    "for i in range(len(episode_rewards)):\n",
    "    mean_rewards.append(np.array(episode_rewards)[max(0, i-100):i+1].mean())\n",
    "    mean_rewards_last10.append(np.array(episode_rewards)[max(0, i-10):i+1].mean())\n",
    "for i in range(len(test_episode_rewards)):\n",
    "    mean_test_rewards.append(np.array(test_episode_rewards)[max(0, i-10):i+1].mean())\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "# plt.plot(episode_steps, episode_rewards, label='Train Reward', alpha=0.6)\n",
    "# plt.plot(test_steps, test_episode_rewards, label='Test Reward', color='red', linewidth=2)\n",
    "# plt.plot(train_steps, mean_rewards_last10, label='Mean Rewards (Last 10)')#, color = 'green')\n",
    "plt.plot(train_steps, mean_rewards, label='Mean Rewards (Last 100)')#, linestyle='--', color = 'orange')\n",
    "plt.plot(test_steps, mean_test_rewards, label='Mean Test Rewards (Last 10)')#, linestyle='--', color='red')\n",
    "plt.scatter(test_steps, test_episode_rewards, label='Test Rewards', alpha=0.6, s=20, color = 'green')#, linestyle='--', color='red')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Train vs Test Rewards over Steps')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "# plt.plot(episode_steps, episode_rewards, label='Train Reward', alpha=0.6)\n",
    "# plt.plot(test_steps, test_episode_rewards, label='Test Reward', color='red', linewidth=2)\n",
    "# plt.plot(train_steps, mean_rewards_last10, label='Mean Rewards (Last 10)')#, color = 'green')\n",
    "plt.plot(train_steps, mean_rewards, label='Mean Rewards (Last 100)')#, linestyle='--', color = 'orange')\n",
    "plt.plot(test_steps, mean_test_rewards, label='Mean Test Rewards (Last 10)')#, linestyle='--', color='red')\n",
    "#plt.plot(test_steps, test_episode_rewards, label='Test Rewards', alpha=0.6)#, linestyle='--', color='red')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Train vs Test Rewards over Steps')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
