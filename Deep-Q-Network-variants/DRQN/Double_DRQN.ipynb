{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf627f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        # env\n",
    "        self.env_name = \"CartPole-v1\"\n",
    "        self.gamma = 0.99\n",
    "        self.num_action = 2\n",
    "        self.state_dim = 4\n",
    "\n",
    "        # replay\n",
    "        self.buffer_size = 1000\n",
    "        self.sequence_length = 10\n",
    "        self.gradient_length = 5 #use last 5 states of sequence sample (10 states) to train (detach first 5 states when feed forward)\n",
    "        self.batch_size = 20\n",
    "\n",
    "        # training\n",
    "        self.total_episodes = 2000\n",
    "        self.learning_rate = 2.3e-3\n",
    "        self.weight_decay = 1e-4\n",
    "        self.start_training_step = 1000\n",
    "        self.train_frequency = 4\n",
    "        self.epochs = 1\n",
    "        self.test_frequency = 10\n",
    "        self.num_test_episodes = 10\n",
    "        self.save_frequency = 1000\n",
    "        self.save_path = 'best_model.pth'\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        ## target network\n",
    "        self.use_soft_update = True\n",
    "        self.update_frequency = 100\n",
    "        self.tau = 0.005\n",
    "\n",
    "        ## episode\n",
    "        self.init_epsilon = 1.\n",
    "        self.end_epsilon = 0.04\n",
    "        self.exploration_fraction = 0.16\n",
    "        self.decay_step = self.total_episodes * self.exploration_fraction\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3fe900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode_Data():\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.next_states.append(next_state)\n",
    "        self.dones.append(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bcfe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_Buffer():\n",
    "    def __init__(self, buffer_size, sequence_length):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.real_size = 0\n",
    "        self.index = 0\n",
    "\n",
    "        self.data = []\n",
    "\n",
    "    def add(self, data):\n",
    "        if len(data.actions) < self.sequence_length:\n",
    "            return\n",
    "        self.data.append(data)\n",
    "        if self.real_size == self.buffer_size:\n",
    "            self.data.pop(0)\n",
    "\n",
    "        self.real_size = min(self.real_size+1, self.buffer_size)\n",
    "        self.index = (self.index+1) % self.buffer_size\n",
    "\n",
    "    def sample(self, batchsize, sequence_length):\n",
    "        episode_idxs = np.random.choice(self.real_size, batchsize, replace=False).astype(np.int64)\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        for episode_idx in episode_idxs:\n",
    "            start_idx = np.random.randint(0, len(self.data[episode_idx].actions)-sequence_length+1, size = 1)[0]\n",
    "            end_idx = start_idx + sequence_length\n",
    "\n",
    "            states.append(np.array(self.data[episode_idx].states[start_idx:end_idx]))\n",
    "            actions.append(np.array(self.data[episode_idx].actions[start_idx:end_idx]))\n",
    "            rewards.append(np.array(self.data[episode_idx].rewards[start_idx:end_idx]))\n",
    "            next_states.append(np.array(self.data[episode_idx].next_states[start_idx:end_idx]))\n",
    "            dones.append(np.array(self.data[episode_idx].dones[start_idx:end_idx]))\n",
    "\n",
    "        return torch.tensor(np.array(states)).transpose(1, 0), \\\n",
    "                torch.tensor(np.array(actions)).transpose(1, 0), \\\n",
    "                torch.tensor(np.array(rewards)).transpose(1, 0), \\\n",
    "                torch.tensor(np.array(next_states)).transpose(1, 0), \\\n",
    "                torch.tensor(np.array(dones)).transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, state_dim=4, num_action=2):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(state_dim, 64)\n",
    "        self.lstm = nn.LSTMCell(64, 64)\n",
    "        self.Q = nn.Linear(64, num_action)\n",
    "\n",
    "    def forward(self, state, h, c):\n",
    "        x = F.relu(self.fc(state))\n",
    "        h, c = self.lstm(x, (h, c))\n",
    "        x = F.relu(h)\n",
    "        Q = self.Q(x)\n",
    "        return Q, h, c\n",
    "    \n",
    "    def init_hidden_state(self, batchsize):\n",
    "        return torch.zeros((batchsize, 64)), torch.zeros((batchsize, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a67ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, model, target_model, env, env_name, replay_buffer, num_action, batch_size, sequence_length, \n",
    "                 gradient_length, learning_rate, weight_decay, gamma, total_episodes,\n",
    "                 train_frequency, epochs, test_frequency, save_frequency, num_test_episodes, start_training_step, save_path, use_soft_update, \n",
    "                 tau, init_epsilon, decay_step, end_epsilon, update_frequency, device):\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.target_model = target_model\n",
    "        self.use_soft_update = use_soft_update\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.gradient_length = gradient_length\n",
    "        self.update_frequency = update_frequency\n",
    "\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.gamma = gamma\n",
    "        self.num_action = num_action\n",
    "\n",
    "        self.total_episodes = total_episodes\n",
    "        self.train_frequency = train_frequency\n",
    "        self.epochs = epochs\n",
    "        self.test_frequency = test_frequency\n",
    "        self.save_frequency = save_frequency\n",
    "        self.num_test_episodes = num_test_episodes\n",
    "        self.save_path = save_path\n",
    "        self.start_training_step = start_training_step\n",
    "\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.decay_step = decay_step\n",
    "        self.end_epsilon = end_epsilon\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def update_weights(self):\n",
    "        if self.use_soft_update:\n",
    "            with torch.no_grad():\n",
    "                for target_param, online_param in zip(self.target_model.parameters(), self.model.parameters()):\n",
    "                    target_param.data.copy_(self.tau * online_param.data + (1.0 - self.tau) * target_param.data)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                for target_param, online_param in zip(self.target_model.parameters(), self.model.parameters()):\n",
    "                    target_param.data.copy_(online_param.data)\n",
    "\n",
    "    def select_action(self, state, h, c, epsilon):\n",
    "        state = torch.tensor(np.array(state)).unsqueeze(0).to(config.device)\n",
    "        with torch.no_grad():\n",
    "            Qs, h, c = self.model(state, h, c)\n",
    "            Qs = Qs[0]\n",
    "        if np.random.rand() > epsilon:\n",
    "            action = Qs.argmax().item()\n",
    "        else:\n",
    "            action = np.random.choice(self.num_action, 1)[0]\n",
    "        return action, h, c\n",
    "    \n",
    "    def decay_epsilon(self, step):\n",
    "        if step < self.decay_step:\n",
    "            epsilon = self.init_epsilon - (step / self.decay_step) * (self.init_epsilon - self.end_epsilon)\n",
    "        else:\n",
    "            epsilon = self.end_epsilon\n",
    "        return epsilon\n",
    "    \n",
    "    def train(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size, self.sequence_length)\n",
    "        h, c = self.model.init_hidden_state(self.batch_size)\n",
    "        h_, c_ = self.target_model.init_hidden_state(self.batch_size)\n",
    "        online_h, oneline_c = self.model.init_hidden_state(self.batch_size)\n",
    "        idxs = torch.arange(0, self.batch_size)\n",
    "        loss = 0.\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(zip(states, actions, rewards, next_states, dones)):\n",
    "            with torch.no_grad():\n",
    "                next_Qs, h_, c_ = self.target_model(next_state, h_, c_)\n",
    "                next_Qs_online, online_h, oneline_c = self.model(next_state, online_h, oneline_c)\n",
    "\n",
    "            if i <= self.sequence_length - self.gradient_length:\n",
    "                with torch.no_grad():\n",
    "                    Qs, h, c = self.model(state, h, c)\n",
    "        else:\n",
    "            Qs, h, c = self.model(state, h, c)\n",
    "            Qs = Qs[idxs, action].reshape(-1)\n",
    "\n",
    "            next_actions = next_Qs_online.argmax(-1)\n",
    "            next_Qs = next_Qs[idxs, next_actions].reshape(-1)\n",
    "            targets = reward + self.gamma * (1 - done.float()) * next_Qs\n",
    "\n",
    "            loss += ((targets - Qs)**2).mean()\n",
    "        loss /= self.gradient_length\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "        \n",
    "    def test(self):\n",
    "        env = gym.make(self.env_name, render_mode=\"rgb_array\")\n",
    "        state, info = env.reset()\n",
    "        total_rewards = 0.\n",
    "        step = 0.\n",
    "        \n",
    "        done = False\n",
    "        h, c = self.model.init_hidden_state(1)\n",
    "        while not done:\n",
    "            action, h, c = self.select_action(state, h, c, -1)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_rewards += reward\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        if total_rewards > self.max_test_rewards:\n",
    "            self.max_test_rewards = total_rewards\n",
    "            torch.save(self.model.state_dict(), self.save_path)\n",
    "        \n",
    "        env.close()\n",
    "        return total_rewards, step\n",
    "    \n",
    "    def learn(self):\n",
    "        self.update_weights()\n",
    "        self.current_step = 0\n",
    "        epsilon = self.init_epsilon\n",
    "\n",
    "        self.max_test_rewards = 0.\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "        self.max_episode_rewards = 0.\n",
    "        self.test_episode_rewards = []\n",
    "        self.test_episode_steps = []\n",
    "        self.losses = []\n",
    "\n",
    "        for episode in range(1, self.total_episodes+1):\n",
    "            start_time = datetime.now()\n",
    "            state, info = self.env.reset()\n",
    "            done = False\n",
    "            total_rewards = 0.\n",
    "            episode_step = 0.\n",
    "\n",
    "            episode_data = Episode_Data()\n",
    "            h, c = self.model.init_hidden_state(1)\n",
    "\n",
    "            while not done:\n",
    "                self.current_step += 1\n",
    "                episode_step += 1\n",
    "                action, h, c = self.select_action(state, h, c, epsilon)\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                total_rewards += reward\n",
    "                done = terminated or truncated\n",
    "                episode_data.add(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                if self.current_step % self.train_frequency == 0 and self.current_step >= self.start_training_step and self.replay_buffer.real_size > self.batch_size:\n",
    "                    for _ in range(self.epochs):\n",
    "                        loss = self.train()\n",
    "                        self.losses.append(loss)\n",
    "                if self.current_step % self.update_frequency == 0 or self.use_soft_update:\n",
    "                    self.update_weights()\n",
    "\n",
    "            self.replay_buffer.add(episode_data)\n",
    "\n",
    "            self.episode_rewards.append(total_rewards)\n",
    "            self.episode_steps.append(episode_step)\n",
    "            self.max_episode_rewards = max(self.max_episode_rewards, total_rewards)\n",
    "            if episode % 100 == 0:\n",
    "                print(f\"Step {self.current_step}, Episode {episode}: Steps: {episode_step}, Rewards: {total_rewards}, \"\n",
    "                      f\"Mean_Rewards: {np.array(self.episode_rewards[-min(100, len(self.episode_rewards)):]).mean():.4f}, \"\n",
    "                      f\"Max_Rewards: {self.max_episode_rewards}, Loss: {np.array(self.losses[-min(1000, len(self.losses)):]).mean():.4f}, \"\n",
    "                      f\"Duration: {datetime.now() - start_time}, epsilon: {epsilon:.6f}\")        \n",
    "\n",
    "            if episode % self.test_frequency == 0:\n",
    "                start_time = datetime.now()\n",
    "                for _ in range(self.num_test_episodes):\n",
    "                    test_rewards, test_steps = self.test()\n",
    "                    self.test_episode_rewards.append(test_rewards)\n",
    "                    self.test_episode_steps.append(test_steps)\n",
    "                if episode % 100 == 0:\n",
    "                    print(f\"Test Episode: {len(self.test_episode_rewards)} Mean Rewards: {np.array(self.test_episode_rewards[-self.num_test_episodes:]).mean():.4f}, \"\n",
    "                          f\"Max_Test_Rewards: {self.max_test_rewards}, Duration: {datetime.now() - start_time}\")\n",
    "            if episode % self.save_frequency == 0:\n",
    "                torch.save(self.model.state_dict(), f\"{episode}.pth\")\n",
    "            epsilon = self.decay_epsilon(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb48997",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = Replay_Buffer(config.buffer_size, config.sequence_length)\n",
    "model = Model(config.state_dim).to(config.device)\n",
    "target_model = Model(config.state_dim).to(config.device)\n",
    "env = gym.make(config.env_name, render_mode=\"rgb_array\")\n",
    "\n",
    "agent = Agent(model, target_model, env, config.env_name, replay_buffer, config.num_action, config.batch_size, config.sequence_length, \n",
    "              config.gradient_length, config.learning_rate, config.weight_decay, config.gamma, config.total_episodes,\n",
    "              config.train_frequency, config.epochs, config.test_frequency, config.save_frequency, config.num_test_episodes, \n",
    "              config.start_training_step, config.save_path, config.use_soft_update, \n",
    "              config.tau, config.init_epsilon, config.decay_step, config.end_epsilon, config.update_frequency, config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4542dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c5f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episode_rewards = agent.episode_rewards\n",
    "test_episode_rewards = agent.test_episode_rewards\n",
    "\n",
    "train_episodes = list(range(1, len(agent.episode_rewards)+1))\n",
    "test_episodes = list(range(1, len(agent.test_episode_rewards)+1, 1))\n",
    "test_episodes_per_test = list(range(agent.test_frequency, len(agent.test_episode_rewards)+1, agent.test_frequency))\n",
    "\n",
    "episode_rewards = np.array(episode_rewards)\n",
    "test_episode_rewards = np.array(test_episode_rewards)\n",
    "\n",
    "mean_rewards = []\n",
    "mean_test_rewards_per_test = []\n",
    "mean_test_rewards_l100 = []\n",
    "for i in range(len(episode_rewards)):\n",
    "    mean_rewards.append(episode_rewards[max(0, i-100):i+1].mean())\n",
    "for i in range(len(test_episode_rewards)):\n",
    "    if (i+1) % config.num_test_episodes == 0:\n",
    "        mean_test_rewards_per_test.append(test_episode_rewards[max(0, i-config.num_test_episodes):i+1].mean())\n",
    "    mean_test_rewards_l100.append(test_episode_rewards[max(0, i-100):i+1].mean())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "idx_full_rewards = np.array(test_episode_rewards) == 500\n",
    "idx_full_mean_rewards = np.array(mean_test_rewards_per_test) == 500\n",
    "\n",
    "plt.plot(train_episodes, mean_rewards, label='Train Rewards [Last 100]')\n",
    "plt.plot(test_episodes, mean_test_rewards_l100, label='Test Rewards [Last 100]')\n",
    "plt.scatter(np.array(test_episodes)[idx_full_rewards], \n",
    "            np.array(test_episode_rewards)[idx_full_rewards], label='Test Rewards = 500', alpha=0.6, s=5, color = 'green')\n",
    "plt.scatter(np.array(test_episodes_per_test)[idx_full_mean_rewards], \n",
    "            np.array(mean_test_rewards_per_test)[idx_full_mean_rewards], label='Test Rewards [Last 10] = 500', alpha=0.6, s=50, color = 'red')\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"DRQN\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4813a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
