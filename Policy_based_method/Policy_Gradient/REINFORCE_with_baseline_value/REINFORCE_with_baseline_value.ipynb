{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c6fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499afe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        # env\n",
    "        self.env_name = \"CartPole-v1\"\n",
    "        self.gamma = 0.99\n",
    "        self.num_action = 2\n",
    "        self.state_dim = 4\n",
    "\n",
    "        # training\n",
    "        self.total_episodes = 5000\n",
    "        self.learning_rate = 2.3e-3\n",
    "        self.weight_decay = 1e-4\n",
    "        self.test_frequency = 10\n",
    "        self.num_test_episodes = 10\n",
    "        self.save_frequency = 1000\n",
    "        self.save_path = 'best_model.pth'\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce31f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim=4, num_action=2):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.policy = nn.Linear(64, num_action)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.policy(x)\n",
    "        return logits\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim=4):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.value = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        value = self.value(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49161dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, actor_model, critic_model, env, device, learning_rate, weight_decay, total_episodes, \n",
    "                 gamma, test_frequency, save_frequency, num_test_episodes, save_path):\n",
    "        self.actor_model = actor_model\n",
    "        self.critic_model = critic_model\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.total_episodes = total_episodes\n",
    "        self.test_frequency = test_frequency\n",
    "        self.save_frequency = save_frequency\n",
    "        self.num_test_episodes = num_test_episodes\n",
    "        self.device = device\n",
    "        config.save_path = save_path\n",
    "\n",
    "    def sample_action(self, state, is_testing = False):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(np.array(state), device = self.device).unsqueeze(0)\n",
    "            logits = self.actor_model(state)\n",
    "            if is_testing:\n",
    "                action = logits[0].argmax(-1).item()\n",
    "            else:\n",
    "                probs = torch.softmax(logits, -1)\n",
    "                distribution = torch.distributions.Categorical(probs)\n",
    "                action = distribution.sample().cpu().numpy()[0]\n",
    "        return action\n",
    "\n",
    "    def play_episode(self, is_testing = False):\n",
    "        done = False\n",
    "        state, info = self.env.reset()\n",
    "        episode_reward = 0.\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        while not done:\n",
    "            action = self.sample_action(state, is_testing)\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "        return episode_reward, states, actions, rewards\n",
    "    \n",
    "    def compute_return(self, rewards):\n",
    "        G = 0\n",
    "        returns = []\n",
    "        for reward in rewards[::-1]:\n",
    "            G = G * self.gamma + reward\n",
    "            returns.append(G)\n",
    "        return returns[::-1]\n",
    "    \n",
    "    def train(self, states, actions, returns):\n",
    "        states = torch.tensor(np.array(states), device = self.device)\n",
    "        returns = torch.tensor(np.array(returns), device = self.device).float()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "\n",
    "        logits = self.actor_model(states)\n",
    "        values = self.critic_model(states)\n",
    "        probs = torch.softmax(logits, -1)\n",
    "\n",
    "        index = torch.arange(0, len(returns), device = self.device)\n",
    "        actions = torch.tensor(np.array(actions), device = self.device, dtype = torch.int64)\n",
    "\n",
    "        loss_P = -((probs[index, actions] + 1e-9).log() * (returns - values.detach().reshape(-1))).mean()\n",
    "        loss_V = torch.nn.functional.mse_loss(values.reshape(-1), returns)\n",
    "\n",
    "        loss_P.backward()\n",
    "        loss_V.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        return loss_P.item(), loss_V.item()\n",
    "\n",
    "    def learn(self):\n",
    "        self.episode_rewards = []\n",
    "        self.test_episode_rewards = []\n",
    "        self.losses_P = []\n",
    "        self.losses_V = []\n",
    "        max_test_rewards = 0.\n",
    "\n",
    "        for episode in range(1, self.total_episodes+1):\n",
    "            start_time = datetime.now()\n",
    "            episode_reward, states, actions, rewards = self.play_episode()\n",
    "            returns = self.compute_return(rewards)\n",
    "            loss_P, loss_V = self.train(states, actions, returns)\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.losses_P.append(loss_P)\n",
    "            self.losses_V.append(loss_V)\n",
    "            if len(self.losses_P) > 100:\n",
    "                self.losses_P.pop(0)\n",
    "                self.losses_V.pop(0)\n",
    "\n",
    "            if episode % 100 == 0:\n",
    "                print(f\"Episode: {episode}, Reward: {episode_reward}, Mean Reward [Last 100]: {np.array(self.episode_rewards)[-min(100, episode):].mean():.4f}, \"\n",
    "                      f\"Max Reward: {max(self.episode_rewards)}, Loss P: {np.array(self.losses_P).mean():.4f}, Loss V: {np.array(self.losses_V).mean():.4f}, \"\n",
    "                      f\"Episode Time: {datetime.now() - start_time}\")\n",
    "                \n",
    "            if episode % self.test_frequency == 0:\n",
    "                start_time = datetime.now()\n",
    "                for _ in range(1, self.num_test_episodes+1):\n",
    "                    test_episode_reward, _, _, _ = self.play_episode(is_testing=True)\n",
    "                    self.test_episode_rewards.append(test_episode_reward)\n",
    "                mean_test_rewards = np.array(self.test_episode_rewards[-self.test_frequency:]).mean()\n",
    "                if mean_test_rewards > max_test_rewards:\n",
    "                    max_test_rewards = mean_test_rewards\n",
    "                    torch.save(self.actor_model.state_dict(), config.save_path)\n",
    "                if len(self.test_episode_rewards) % 100 == 0:\n",
    "                    print(f\"Test Episode: {len(self.test_episode_rewards)}, Reward: {test_episode_reward}, \"\n",
    "                          f\"Mean Reward [Last {self.num_test_episodes}]: {mean_test_rewards:.4f}, \"\n",
    "                          f\"Max Reward: {max_test_rewards}, Test Time: {datetime.now() - start_time}\")\n",
    "                \n",
    "            if episode % self.save_frequency == 0:\n",
    "                torch.save(self.actor_model.state_dict(), f\"{episode}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model = Actor(config.state_dim, config.num_action)\n",
    "critic_model = Critic(config.state_dim)\n",
    "env = gym.make(config.env_name, render_mode=\"rgb_array\")\n",
    "agent = Agent(actor_model, critic_model, env, config.device, config.learning_rate, config.weight_decay, config.total_episodes,\n",
    "              config.gamma, config.test_frequency, config.save_frequency, config.num_test_episodes, config.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47324abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b46d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episode_rewards = agent.episode_rewards\n",
    "test_episode_rewards = agent.test_episode_rewards\n",
    "\n",
    "train_episodes = list(range(1, len(agent.episode_rewards)+1))\n",
    "test_episodes = list(range(1, len(agent.test_episode_rewards)+1, 1))\n",
    "test_episodes_per_test = list(range(10, len(agent.test_episode_rewards)+1, 10))\n",
    "\n",
    "episode_rewards = np.array(episode_rewards)\n",
    "test_episode_rewards = np.array(test_episode_rewards)\n",
    "\n",
    "mean_rewards = []\n",
    "mean_test_rewards_per_test = []\n",
    "mean_test_rewards_l100 = []\n",
    "for i in range(len(episode_rewards)):\n",
    "    mean_rewards.append(episode_rewards[max(0, i-100):i+1].mean())\n",
    "for i in range(len(test_episode_rewards)):\n",
    "    if (i+1) % config.num_test_episodes == 0:\n",
    "        mean_test_rewards_per_test.append(test_episode_rewards[max(0, i-config.num_test_episodes):i+1].mean())\n",
    "    mean_test_rewards_l100.append(test_episode_rewards[max(0, i-100):i+1].mean())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "idx_full_rewards = np.array(test_episode_rewards) == 500\n",
    "idx_full_mean_rewards = np.array(mean_test_rewards_per_test) == 500\n",
    "\n",
    "plt.plot(train_episodes, mean_rewards, label='Train Rewards [Last 100]')\n",
    "plt.plot(test_episodes, mean_test_rewards_l100, label='Test Rewards [Last 100]')\n",
    "plt.scatter(np.array(test_episodes)[idx_full_rewards], \n",
    "            np.array(test_episode_rewards)[idx_full_rewards], label='Test Rewards = 500', alpha=0.6, s=5, color = 'green')\n",
    "plt.scatter(np.array(test_episodes_per_test)[idx_full_mean_rewards], \n",
    "            np.array(mean_test_rewards_per_test)[idx_full_mean_rewards], label='Test Rewards [Last 10] = 500', alpha=0.6, s=50, color = 'red')\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"REINFORCE with baseline\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c90910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episode_rewards = agent.episode_rewards[:1000]\n",
    "test_episode_rewards = agent.test_episode_rewards[:1000]\n",
    "\n",
    "train_episodes = list(range(1, len(episode_rewards)+1))\n",
    "test_episodes = list(range(1, len(test_episode_rewards)+1, 1))\n",
    "test_episodes_per_test = list(range(10, len(test_episode_rewards)+1, 10))\n",
    "\n",
    "episode_rewards = np.array(episode_rewards)\n",
    "test_episode_rewards = np.array(test_episode_rewards)\n",
    "\n",
    "mean_rewards = []\n",
    "mean_test_rewards_per_test = []\n",
    "mean_test_rewards_l100 = []\n",
    "for i in range(len(episode_rewards)):\n",
    "    mean_rewards.append(episode_rewards[max(0, i-100):i+1].mean())\n",
    "for i in range(len(test_episode_rewards)):\n",
    "    if (i+1) % config.num_test_episodes == 0:\n",
    "        mean_test_rewards_per_test.append(test_episode_rewards[max(0, i-config.num_test_episodes):i+1].mean())\n",
    "    mean_test_rewards_l100.append(test_episode_rewards[max(0, i-100):i+1].mean())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "idx_full_rewards = np.array(test_episode_rewards) == 500\n",
    "idx_full_mean_rewards = np.array(mean_test_rewards_per_test) == 500\n",
    "\n",
    "plt.plot(train_episodes, mean_rewards, label='Train Rewards [Last 100]')\n",
    "plt.plot(test_episodes, mean_test_rewards_l100, label='Test Rewards [Last 100]')\n",
    "plt.scatter(np.array(test_episodes)[idx_full_rewards], \n",
    "            np.array(test_episode_rewards)[idx_full_rewards], label='Test Rewards = 500', alpha=0.6, s=5, color = 'green')\n",
    "plt.scatter(np.array(test_episodes_per_test)[idx_full_mean_rewards], \n",
    "            np.array(mean_test_rewards_per_test)[idx_full_mean_rewards], label='Test Rewards [Last 10] = 500', alpha=0.6, s=50, color = 'red')\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"REINFORCE with baseline\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962bd713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
