{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CalAHcRDwS0"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDoqT-vGxI3y"
      },
      "outputs": [],
      "source": [
        "class Config():\n",
        "    def __init__(self):\n",
        "        #env\n",
        "        self.env_name = \"CartPole-v1\"\n",
        "        self.total_episode = 1000\n",
        "        self.gamma = 0.997\n",
        "        self.num_action = 2\n",
        "        self.state_dim = 4\n",
        "\n",
        "        #MCTS\n",
        "        self.reuse_tree = False #reuse or build new tree. After first MCTS (for s0), we have tree for s1, ... We can reuse this tree or build new tree for s1, s2, ...\n",
        "        #You need set config.timeout or config.search_step to None and another to integer\n",
        "        self.timeout = None #each MCTS step will run until timeout (set config.timeout to None if you don't want limit by time)\n",
        "        self.search_step = 50 #each MCTS step will run config.search_step (set config.search_step to None if you want to run with time limit)\n",
        "        self.max_moves = 512\n",
        "        self.num_sampling_moves = 501\n",
        "\n",
        "        # Root prior exploration noise.\n",
        "        self.root_dirichlet_alpha = 0.3\n",
        "        self.root_exploration_fraction = 0.25\n",
        "\n",
        "        # UCB formula\n",
        "        self.pb_c_base = 19652\n",
        "        self.pb_c_init = 1.25\n",
        "\n",
        "        #training\n",
        "        self.training_steps = 10\n",
        "        self.testing_steps = 10\n",
        "        self.total_test_episode = 10\n",
        "        self.start_train_from_episode = 10\n",
        "        self.buffer_size = 256\n",
        "        self.save_step = 100\n",
        "        self.epochs = 100\n",
        "        self.V_loss_type = \"CE\" # or \"MSE\"\n",
        "\n",
        "        #model\n",
        "        self.learning_rate = 5e-3\n",
        "        self.use_scheduler = False\n",
        "        self.final_lr = 5e-4\n",
        "        self.total_decay_step = 500\n",
        "        self.weight_decay  = 1e-4\n",
        "        self.batchsize = 256\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.max_support_value = 500\n",
        "        self.support_step = self.max_support_value//50\n",
        "        self.save_path = \"best_CE.pth\"\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MinMaxScale():\n",
        "    def __init__(self):\n",
        "        self.min_G = 500.\n",
        "        self.max_G = 0.\n",
        "\n",
        "    def update(self, G):\n",
        "        self.min_G = min(self.min_G, G)\n",
        "        self.max_G = max(self.max_G, G)\n",
        "\n",
        "    def scale(self, G):\n",
        "        if self.min_G < self.max_G:\n",
        "            return (G - self.min_G) / (self.max_G - self.min_G)\n",
        "        return G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scalar_to_support(value, support_values):\n",
        "    \"\"\"\n",
        "    value: [batch_size], float\n",
        "    support_values: 1D tensor like [0, 50, ..., 500] of shape [N]\n",
        "    return: [batch_size, N] soft distribution\n",
        "    \"\"\"\n",
        "    batch_size = value.size(0)\n",
        "    device = value.device\n",
        "    N = support_values.size(0)\n",
        "\n",
        "    # Clamp values to be within support range\n",
        "    min_v, max_v = support_values[0], support_values[-1]\n",
        "    value = torch.clamp(value, min_v.item(), max_v.item())\n",
        "\n",
        "    # Find which two support points the value lies between\n",
        "    diff = support_values.unsqueeze(0) - value.unsqueeze(1)  # [batch_size, N]\n",
        "    mask = diff <= 0\n",
        "    idx_low = mask.sum(dim=1) - 1  # Index of largest support_value <= value\n",
        "    idx_high = idx_low + 1\n",
        "\n",
        "    idx_low = idx_low.clamp(0, N - 1)\n",
        "    idx_high = idx_high.clamp(0, N - 1)\n",
        "\n",
        "    v_low = support_values[idx_low]\n",
        "    v_high = support_values[idx_high]\n",
        "\n",
        "    # Linear interpolation\n",
        "    weight_high = (value - v_low) / (v_high - v_low + 1e-8)\n",
        "    weight_low = 1.0 - weight_high\n",
        "\n",
        "    support = torch.zeros((batch_size, N), device=device)\n",
        "    support.scatter_(1, idx_low.unsqueeze(1), weight_low.unsqueeze(1))\n",
        "    support.scatter_add_(1, idx_high.unsqueeze(1), weight_high.unsqueeze(1))\n",
        "\n",
        "    return support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def support_to_scalar(probs, support_values):\n",
        "    \"\"\"\n",
        "    probs: [batch_size, N], soft distribution\n",
        "    support_values: [N]\n",
        "    return: [batch_size]\n",
        "    \"\"\"\n",
        "    return torch.sum(probs * support_values.unsqueeze(0), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxBxgYQNENNG"
      },
      "outputs": [],
      "source": [
        "class Policy_Model(nn.Module):\n",
        "    def __init__(self, state_dim=4, action_dim=2):\n",
        "        super(Policy_Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "\n",
        "        # Policy head\n",
        "        self.policy_head = nn.Linear(64, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        policy_logits = self.policy_head(x)\n",
        "        return policy_logits\n",
        "\n",
        "class Value_Model(nn.Module):\n",
        "    def __init__(self, state_dim=4, support_values=2, loss_type = \"CE\"):\n",
        "        super(Value_Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "\n",
        "        self.loss_type = loss_type\n",
        "        # Value head\n",
        "        if self.loss_type == \"CE\":\n",
        "            self.value_head = nn.Linear(64, len(support_values))\n",
        "            self.support_values = support_values\n",
        "        else:\n",
        "            self.value_head = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        value_logit = self.value_head(x)\n",
        "        if self.loss_type == \"CE\":\n",
        "            with torch.no_grad():\n",
        "                value = support_to_scalar(F.softmax(value_logit, -1), self.support_values.to(value_logit.device)).unsqueeze(1)\n",
        "        else:\n",
        "            value = value_logit\n",
        "        return value, value_logit\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, state_dim=4, action_dim=2, support_values = None, loss_type = \"CE\"):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Policy Model\n",
        "        self.policy_model = Policy_Model(state_dim, action_dim)\n",
        "\n",
        "        # Value model\n",
        "        self.value_model = Value_Model(state_dim, support_values, loss_type)\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.policy_model(state), self.value_model(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyTzHo-rRYYZ"
      },
      "outputs": [],
      "source": [
        "class Episode():\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.total_rewards = []\n",
        "        self.values = []\n",
        "        self.probs = []\n",
        "\n",
        "    def save_step_data(self, state, action, reward, done, value, probs):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "        self.values.append(value)\n",
        "        self.probs.append(probs)\n",
        "\n",
        "    def save_total_rewards(self):\n",
        "        total_rewards = 0.\n",
        "        for reward in self.rewards[::-1]:\n",
        "            total_rewards = total_rewards * config.gamma + reward\n",
        "            self.total_rewards.append(total_rewards)\n",
        "        self.total_rewards.reverse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3PrH5qNRD2N"
      },
      "outputs": [],
      "source": [
        "class Replay_Buffer():\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.episodes = []\n",
        "        self.index = 0\n",
        "        self.real_size = 0\n",
        "\n",
        "    def add(self, episode):\n",
        "        if self.real_size < self.buffer_size:\n",
        "            self.episodes.append(episode)\n",
        "        else:\n",
        "            self.episodes[self.index] = episode\n",
        "        self.index = (self.index + 1) % self.buffer_size\n",
        "        self.real_size = min(self.real_size + 1, self.buffer_size)\n",
        "\n",
        "    def sample(self, batchsize):\n",
        "        episode_index = np.random.choice(self.real_size, batchsize, replace = True)\n",
        "        states = []\n",
        "        target_values = []\n",
        "        target_probs = []\n",
        "\n",
        "        for i in range(batchsize):\n",
        "            episode = self.episodes[episode_index[i]]\n",
        "            len_episode = len(episode.states)\n",
        "            pos_idx = np.random.choice(len_episode, 1, replace = False)[0]\n",
        "            states.append(episode.states[pos_idx])\n",
        "            target_values.append(episode.values[pos_idx])\n",
        "            target_probs.append(episode.probs[pos_idx])\n",
        "\n",
        "        return states, target_probs, target_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY0mlp7a0ZCv"
      },
      "outputs": [],
      "source": [
        "class Node():\n",
        "    def __init__(self, prior, num_action, base_env, parent = None, lead_action = None, state = None):\n",
        "        self.N = 0\n",
        "        self.V = 0.\n",
        "\n",
        "        self.prior = prior\n",
        "\n",
        "        self.predict_V = 0.\n",
        "        self.predict_P = None\n",
        "\n",
        "        self.children = None\n",
        "        self.parent = parent\n",
        "        self.lead_action = lead_action\n",
        "        self.num_action = num_action\n",
        "\n",
        "        self.env = copy.deepcopy(base_env)\n",
        "        if lead_action is not None:\n",
        "            self.state, self.reward, terminated, truncated, info = self.env.step(lead_action)\n",
        "            self.done = terminated or truncated\n",
        "        else:\n",
        "            self.done = False\n",
        "            self.reward = 0.\n",
        "            self.state = state\n",
        "        self.available_actions = set(list(range(num_action))) if not self.done else set()\n",
        "\n",
        "    def ucb_score(self, min_max_scale):\n",
        "        if self.N == 0:\n",
        "            return 1e9\n",
        "\n",
        "        pb_c = np.log((self.parent.N + config.pb_c_base + 1) /\n",
        "                        config.pb_c_base) + config.pb_c_init\n",
        "        pb_c *= np.sqrt(self.parent.N) / (self.N + 1)\n",
        "\n",
        "        prior_score = pb_c * self.prior\n",
        "        value_score = self.V / self.N\n",
        "        value_score = min_max_scale.scale(value_score)\n",
        "        return prior_score + value_score\n",
        "\n",
        "    def is_explore(self):\n",
        "        return self.children and not self.done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bi8DR4H80anO"
      },
      "outputs": [],
      "source": [
        "def add_exploration_noise(node):\n",
        "  actions = np.arange(config.num_action)\n",
        "  noise = np.random.gamma(config.root_dirichlet_alpha, 1, len(actions))\n",
        "  frac = config.root_exploration_fraction\n",
        "  for a, n in zip(actions, noise):\n",
        "    node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "def simulate(model, node):\n",
        "    # env = copy.deepcopy(node.env)\n",
        "    # done = node.done\n",
        "    # G = 0.\n",
        "    # rewards = []\n",
        "    # while not done:\n",
        "    #     action = np.random.choice(config.num_action)\n",
        "    #     _, reward, terminated, truncated, info = env.step(action)\n",
        "    #     done = terminated or truncated\n",
        "    #     rewards.append(reward)\n",
        "    # for reward in reversed(rewards):\n",
        "    #     G = G * config.gamma + reward\n",
        "    # env.close()\n",
        "    if node.done is True:\n",
        "        return torch.tensor([1./config.num_action for _ in range(config.num_action)]), torch.tensor([0.])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        p, (V, _) = model(torch.tensor(np.array(node.state, dtype = np.float32), device = config.device).unsqueeze(0))\n",
        "        p = F.softmax(p, -1)\n",
        "    return p, V#G\n",
        "\n",
        "def create_child(model, node):\n",
        "    node.children = []\n",
        "    with torch.no_grad():\n",
        "        p, _ = model(torch.tensor(np.array(node.state, dtype = np.float32), device = config.device).unsqueeze(0))\n",
        "        p = F.softmax(p, -1)\n",
        "    for action in range(config.num_action):\n",
        "        new_node = Node(p[0][action].item(), config.num_action, node.env, node, action, None)\n",
        "        node.children.append(new_node)\n",
        "    return node\n",
        "\n",
        "def backpropagate(node, p, V, min_max_scale):\n",
        "    while node:\n",
        "        node.N += 1\n",
        "        node.V += V\n",
        "        V = config.gamma * V + node.reward\n",
        "        min_max_scale.update(node.V / node.N)\n",
        "        node = node.parent\n",
        "\n",
        "def find_explore_node(explore_node, min_max_scale):\n",
        "    current_node = explore_node\n",
        "    while current_node.children and not current_node.done:\n",
        "        children = current_node.children\n",
        "        actions_score = [child.ucb_score(min_max_scale) for child in children]\n",
        "        max_score = max(actions_score)\n",
        "        best_children = [child for child, score in zip(children, actions_score) if score == max_score]\n",
        "        idx = np.random.choice(len(best_children))\n",
        "        current_node = best_children[idx]\n",
        "    return current_node\n",
        "\n",
        "def explore(model, explore_node, min_max_scale, is_test):\n",
        "    current_node = find_explore_node(explore_node, min_max_scale)\n",
        "\n",
        "    if current_node.children is None and not current_node.done:\n",
        "        current_node = create_child(model, current_node)\n",
        "        if current_node.lead_action is None and not is_test:\n",
        "            add_exploration_noise(current_node)\n",
        "\n",
        "    _, V = simulate(model, current_node)\n",
        "\n",
        "    backpropagate(current_node, _, V.item(), min_max_scale)\n",
        "\n",
        "def check_mcts(timeout, start_time, search_step, current_step):\n",
        "    if timeout is None:\n",
        "        return current_step < search_step\n",
        "    return datetime.now() - start_time < timedelta(seconds=timeout)\n",
        "\n",
        "def select_action(root_node, episode_data, is_test):\n",
        "    if len(episode_data.actions) < config.num_sampling_moves and not is_test:\n",
        "        #random with temp\n",
        "        children = root_node.children\n",
        "        Ns= np.array([child.N for child in children])\n",
        "        probs = Ns / Ns.sum()\n",
        "        action = np.random.choice(len(probs), 1, p=probs)[0]\n",
        "        next_children = children[action]\n",
        "        return action, probs, root_node, next_children\n",
        "    else:\n",
        "        children = root_node.children\n",
        "        Ns= np.array([child.N for child in children])\n",
        "        max_N = max(Ns)\n",
        "        probs = Ns / Ns.sum()\n",
        "        best_children = [child for child, action, N in zip(children, [child.lead_action for child in children], Ns) if N == max_N]\n",
        "        best_actions = [action for child, action, N in zip(children, [child.lead_action for child in children], Ns) if N == max_N]\n",
        "        best_child = best_children[0]\n",
        "        best_child.parent = None\n",
        "        best_child.lead_action = None\n",
        "        best_child.reward = 0.\n",
        "        best_child.done = False\n",
        "        return best_actions[0], probs, root_node, best_child\n",
        "\n",
        "def mcts(model, episode_data, root_state, root_env, timeout = 1, search_step = None, tree=None, is_test = False):\n",
        "    min_max_scale = MinMaxScale()\n",
        "    root_node = tree if (tree and config.reuse_tree) else Node(0, config.num_action, root_env, None, None, state = root_state)\n",
        "    start_time = datetime.now()\n",
        "    step = 0\n",
        "    while check_mcts(timeout, start_time, search_step, step):\n",
        "        explore(model, root_node, min_max_scale, is_test)\n",
        "        step += 1\n",
        "    return select_action(root_node, episode_data, is_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nVIcL-5Qt2K"
      },
      "outputs": [],
      "source": [
        "def play_episode(model, replay_buffer, is_test):\n",
        "    env = gym.make(config.env_name, render_mode=\"rgb_array\")\n",
        "    state, info = env.reset()\n",
        "\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    episode_step = 0\n",
        "    next_tree = None\n",
        "    tree = None\n",
        "\n",
        "    episode_data = Episode()\n",
        "    action = None\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        next_action, probs, tree, next_tree = mcts(model, episode_data, state, env, config.timeout, config.search_step, next_tree, is_test)\n",
        "        next_state, reward, terminated, truncated, info = env.step(next_action)\n",
        "        episode_reward += reward\n",
        "        episode_step += 1\n",
        "\n",
        "        episode_data.save_step_data(state, action, reward, done, tree.V / tree.N, probs)\n",
        "\n",
        "        action = next_action\n",
        "        state = next_state\n",
        "\n",
        "        done = terminated or truncated\n",
        "\n",
        "    if not is_test:\n",
        "        episode_data.save_total_rewards()\n",
        "        replay_buffer.add(episode_data)\n",
        "\n",
        "    return episode_reward, episode_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaVMETkh8VGp"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, replay_buffer, scheduler, support_values):\n",
        "    optimizer.zero_grad()\n",
        "    states, target_probs, target_values = replay_buffer.sample(config.batchsize)\n",
        "    target_probs = torch.tensor(target_probs, device = config.device)\n",
        "    target_values = torch.tensor(target_values, device = config.device).reshape(-1) #.unsqueeze(1)\n",
        "\n",
        "    probs, (value, value_logit) = model(torch.tensor(np.array(states, dtype = np.float32), device = config.device))\n",
        "    loss_P = F.cross_entropy(probs, target_probs)\n",
        "\n",
        "    if config.V_loss_type == \"CE\":\n",
        "        target_values_prob = scalar_to_support(target_values, support_values.to(target_values.device))\n",
        "        loss_V = F.cross_entropy(value_logit, target_values_prob)\n",
        "        with torch.no_grad():\n",
        "            mse_V = F.mse_loss(value, target_values.unsqueeze(1)).item()\n",
        "    else:\n",
        "        loss_V = F.mse_loss(value, target_values.unsqueeze(1))\n",
        "        mse_V = loss_V.item()\n",
        "\n",
        "    loss = loss_V + loss_P\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if config.use_scheduler:\n",
        "        scheduler.step()\n",
        "    return loss_P.item(), loss_V.item(), mse_V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIasN6Bu0bh2",
        "outputId": "75bb1c5f-4538-4bf7-c10e-c9f7df44b4ad"
      },
      "outputs": [],
      "source": [
        "episode_rewards = []\n",
        "episode_steps = []\n",
        "episode_runtimes = []\n",
        "\n",
        "test_episode_rewards = []\n",
        "test_episode_steps = []\n",
        "test_episode_runtimes = []\n",
        "\n",
        "support_values = torch.arange(0, config.max_support_value + 1, config.support_step).float()\n",
        "\n",
        "model = Model(config.state_dim, config.num_action, support_values, config.V_loss_type)\n",
        "model.to(config.device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "lr_gamma = (config.final_lr / config.learning_rate) ** (1 / config.total_decay_step)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: lr_gamma ** step)\n",
        "\n",
        "replay_buffer = Replay_Buffer(config.buffer_size)\n",
        "\n",
        "max_test_rewards = 0.\n",
        "\n",
        "for episode in range(config.total_episode):\n",
        "    start_time = datetime.now()\n",
        "    episode_reward, episode_step = play_episode(model, replay_buffer, False)\n",
        "\n",
        "    episode_rewards.append(episode_reward)\n",
        "    episode_steps.append(episode_step)\n",
        "    episode_runtimes.append(datetime.now() - start_time)\n",
        "    print(f\"episode: {episode}, rewards: {episode_reward}, episode_time: {episode_runtimes[-1]}, mean_rewards: {np.array(episode_rewards)[-min(100, len(episode_rewards)):].mean():.4f}\")\n",
        "\n",
        "    if episode % config.training_steps == 0 and len(episode_rewards) >= config.start_train_from_episode:\n",
        "        losses_P = []\n",
        "        losses_V = []\n",
        "        mse_Vs = []\n",
        "        for _ in range(config.epochs):\n",
        "            loss_P, loss_V, mse_V = train(model, optimizer, replay_buffer, scheduler, support_values)\n",
        "            losses_P.append(loss_P)\n",
        "            losses_V.append(loss_V)\n",
        "            mse_Vs.append(mse_V)\n",
        "        print(f\"Loss P: {np.array(losses_P).mean():.4f}, Loss V: {np.array(losses_V).mean():.4f}, MSE V: {np.array(mse_Vs).mean():.4f} lr: {optimizer.param_groups[0]['lr']:.6f}\\n\")\n",
        "\n",
        "    if episode % config.save_step == 0:\n",
        "        torch.save(model.state_dict(), f\"{episode}.pth\")\n",
        "\n",
        "    if episode % config.testing_steps == 0 and len(episode_rewards) >= config.start_train_from_episode:\n",
        "        for test_episode in range(config.total_test_episode):\n",
        "            test_time = datetime.now()\n",
        "            test_episode_reward, test_episode_step = play_episode(model, replay_buffer, True)\n",
        "\n",
        "            test_episode_rewards.append(test_episode_reward)\n",
        "            test_episode_steps.append(test_episode_step)\n",
        "            test_episode_runtimes.append(datetime.now() - test_time)\n",
        "            print(f\"test episode: {test_episode}, rewards: {test_episode_reward}, episode_time: {test_episode_runtimes[-1]}, mean_rewards: {np.array(test_episode_rewards[-(test_episode+1):]).mean():.4f}\")\n",
        "        if np.array(test_episode_rewards[-config.total_test_episode:]).mean() > max_test_rewards:\n",
        "            torch.save(model.state_dict(), config.save_path)\n",
        "            max_test_rewards = np.array(test_episode_rewards[-config.total_test_episode:]).mean()\n",
        "        print(\"\\n\")\n",
        "\n",
        "episode_rewards = np.array(episode_rewards)\n",
        "print(episode_rewards)\n",
        "print(episode_rewards.max(), episode_rewards.min(), episode_rewards.mean(), episode_rewards.std())\n",
        "print(max(episode_runtimes), min(episode_runtimes), np.mean(episode_runtimes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnQDufKaV0Zj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mean_rewards = []\n",
        "for i in range(len(episode_rewards)):\n",
        "    mean_rewards.append(np.array(episode_rewards)[max(0, i-100):i+1].mean())\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(episode_rewards, label='Rewards')\n",
        "plt.plot(mean_rewards, label='Mean Rewards', linestyle='--')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Rewards vs Mean Rewards')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gym",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
